{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm numpy scikit-learn networkx bs4 matplotlib requests rank_bm25 openai faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raffukhondaker/Projects/mlatuva/adobe/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from openai import OpenAI\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:01<00:00, 32.25it/s]\n",
      "100%|██████████| 24/24 [00:11<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "course_numbers = [\n",
    "    1043, 1045, 1046, 1047, 1048, 1049, 1050, 1054, 1055, 1056, \n",
    "    1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, \n",
    "    1067, 1068, 1069, 1221\n",
    "]\n",
    "\n",
    "courses = []\n",
    "certificates = []\n",
    "certificate_htmls_location = 'certificate_htmls'\n",
    "\n",
    "for html in tqdm(os.listdir(certificate_htmls_location)):\n",
    "    certificate = Certificate(f'{certificate_htmls_location}/{html}')\n",
    "    certificates.append(certificate)\n",
    "    \n",
    "for n in tqdm(course_numbers):\n",
    "    new_course = Course(f'https://certification.adobe.com/courses/{n}')\n",
    "    courses.append(new_course)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: list[Source] = []\n",
    "doc2source: dict[str, Source] = {}\n",
    "\n",
    "for i, course in enumerate(courses):\n",
    "    documents.append(course.to_text())\n",
    "    doc2source[course.to_text()] = course\n",
    "\n",
    "for i, cert in enumerate(certificates):\n",
    "    documents.append(cert.to_text())\n",
    "    doc2source[cert.to_text()] = cert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "tag2embed = {}\n",
    "\n",
    "# documents = [\n",
    "#     \"Neural networks are a key part of deep learning.\",\n",
    "#     \"BM25 is a ranking function used in search engines.\",\n",
    "#     \"Reinforcement learning is used in decision-making.\",\n",
    "#     \"Graph neural networks process graph-structured data.\"\n",
    "# ]\n",
    "\n",
    "# Keyword Search\n",
    "tokenized_corpus = [doc.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Semantic Search\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_embedding(text):\n",
    "    if text in tag2embed:\n",
    "        return tag2embed[text]\n",
    "    print(\"New embed for:\", text)\n",
    "    embed = client.embeddings.create(input=text, model=\"text-embedding-ada-002\").data[0].embedding\n",
    "    tag2embed[text] = embed\n",
    "    return embed\n",
    "\n",
    "embeddings = np.array([get_embedding(doc) for doc in documents])\n",
    "d = embeddings.shape[1]  # Embedding dimension\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag Search\n",
    "\n",
    "type_e = np.array([get_embedding(doc.type) for doc in doc2source.values()])\n",
    "level_e = np.array([get_embedding(doc.level) for doc in doc2source.values()])\n",
    "category_e = np.array([get_embedding(doc.category) for doc in doc2source.values()])\n",
    "job_e = np.array([get_embedding(doc.job_role) for doc in doc2source.values()])\n",
    "\n",
    "d = embeddings.shape[1]  # Embedding dimension\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Tag name, embedding of tag name, indices\n",
    "tag_embeddings = {\n",
    "    \"type\": faiss.IndexFlatL2(type_e.shape[1]),\n",
    "    \"level\": faiss.IndexFlatL2(level_e.shape[1]),\n",
    "    \"category\": faiss.IndexFlatL2(category_e.shape[1]),\n",
    "    \"job\": faiss.IndexFlatL2(job_e.shape[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 10}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "There are no type variables left in collections.defaultdict[int]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(documents[i], score) \u001b[38;5;28;01mfor\u001b[39;00m i, score \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m    106\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat courses are good for beginner programmers?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 107\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank, (doc, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    110\u001b[0m     source \u001b[38;5;241m=\u001b[39m doc2source[doc]\n",
      "Cell \u001b[0;32mIn[40], line 97\u001b[0m, in \u001b[0;36mhybrid_search\u001b[0;34m(query, bm25, index, tag_embeddings, top_n, k, bm25_weight, tag_weight)\u001b[0m\n\u001b[1;32m     95\u001b[0m bm25_results \u001b[38;5;241m=\u001b[39m keyword_search(query, bm25, top_n\u001b[38;5;241m=\u001b[39mtop_n)\n\u001b[1;32m     96\u001b[0m vector_results \u001b[38;5;241m=\u001b[39m semantic_search(query, index, top_n\u001b[38;5;241m=\u001b[39mtop_n)\n\u001b[0;32m---> 97\u001b[0m tag_results \u001b[38;5;241m=\u001b[39m \u001b[43mtag_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# results = rrf_fusion({\"BM25\": bm25_results, \"Vector\": vector_results}, k=k)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m: bm25_weight, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbm25_weight \u001b[38;5;241m-\u001b[39m tag_weight, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTag\u001b[39m\u001b[38;5;124m\"\u001b[39m: tag_weight}\n",
      "Cell \u001b[0;32mIn[40], line 67\u001b[0m, in \u001b[0;36mtag_search\u001b[0;34m(query, tag_embeddings, top_n)\u001b[0m\n\u001b[1;32m     64\u001b[0m     tag_weights \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;241m/\u001b[39m total_weight \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tag_weights\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Fuse tag results using weighted fusion\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {doc_id: rank \u001b[38;5;28;01mfor\u001b[39;00m doc_id, rank \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "Cell \u001b[0;32mIn[40], line 35\u001b[0m, in \u001b[0;36mweighted_fusion\u001b[0;34m(results, weights)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (method, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m---> 35\u001b[0m     hybrid_results[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weights[method] \u001b[38;5;241m*\u001b[39m score\n\u001b[1;32m     37\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(hybrid_results\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sorted_results\n",
      "\u001b[0;31mTypeError\u001b[0m: There are no type variables left in collections.defaultdict[int]"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def rrf_fusion(results, k=60):\n",
    "    \"\"\"\n",
    "    Computes Reciprocal Rank Fusion (RRF) for multiple ranked lists.\n",
    "    \n",
    "    :param results: Dict of {method_name: {doc_id: rank_position}}\n",
    "    :param k: Small constant (default: 60) for score scaling.\n",
    "    :return: Sorted list of (document_id, RRF score).\n",
    "    \"\"\"\n",
    "    rrf_scores = {}\n",
    "\n",
    "    for method, ranked_docs in results.items():\n",
    "        for doc_id, rank in ranked_docs.items():\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = 0\n",
    "            rrf_scores[doc_id] += 1 / (k + rank)\n",
    "\n",
    "    return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)    \n",
    "\n",
    "def weighted_fusion(results, weights={\"BM25\": 0.5, \"Vector\": 0.5}):\n",
    "    \"\"\"\n",
    "    Performs weighted sum fusion for multiple ranked lists.\n",
    "\n",
    "    :param results: Dict of {method_name: {doc_id: rank_position}}\n",
    "    :param weights: Dict of {method_name: weight}, e.g., {\"BM25\": 0.5, \"Vector\": 0.5}\n",
    "    :return: Sorted list of (document_id, weighted_score).\n",
    "    \"\"\"\n",
    "    hybrid_results = defaultdict[int]\n",
    "    for i, (method, score) in enumerate(results.items()):\n",
    "        print(method, score)\n",
    "        hybrid_results[i] += weights[method] * score\n",
    "    \n",
    "    sorted_results = sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n",
    "\n",
    "def tag_search(query: str, tag_embeddings: dict[str, faiss.IndexFlatL2], top_n=10) -> dict[int, int]:\n",
    "    query_embedding = np.array(get_embedding(query)).reshape(1, -1)\n",
    "    tag_results = {}\n",
    "    tag_weights = {}\n",
    "\n",
    "    for tag in tag_embeddings:\n",
    "        # Compute tag relevance weight\n",
    "        if tag == \"type\":\n",
    "            tag_weight = 1.0  # Hardcoded priority for `type` tags\n",
    "        else:\n",
    "            tag_weight = cosine_similarity(get_embedding(query), get_embedding(tag))\n",
    "\n",
    "        tag_weights[tag] = tag_weight  # Store weight for weighted fusion\n",
    "\n",
    "        # Perform FAISS search for this tag\n",
    "        _, vector_top_n = tag_embeddings[tag].search(query_embedding, top_n)\n",
    "        \n",
    "        # Convert FAISS result to {doc_id: rank_position}\n",
    "        result = {i.item(): rank + 1 for rank, i in enumerate(vector_top_n[0])}  # Rank starts from 1\n",
    "        tag_results[tag] = result  # Store per-tag search results\n",
    "\n",
    "    # Normalize tag weights so they sum to 1\n",
    "    total_weight = sum(tag_weights.values())\n",
    "    if total_weight > 0:\n",
    "        tag_weights = {k: v / total_weight for k, v in tag_weights.items()}\n",
    "    \n",
    "    # Fuse tag results using weighted fusion\n",
    "    results = weighted_fusion(tag_results, tag_weights)\n",
    "\n",
    "    return {doc_id: rank for doc_id, rank in results}  # Return matching output format\n",
    "\n",
    "\n",
    "def keyword_search(query: str, bm25: BM25Okapi, top_n=10) -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Perform BM25 keyword search and return ranked results.\n",
    "    \"\"\"\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_top_n = np.argsort(bm25_scores)[::-1][:top_n]  # Get top BM25 results\n",
    "\n",
    "    return {i.item(): rank+1 for rank, i in enumerate(bm25_top_n)}  # Rank position starts from 1\n",
    "\n",
    "def semantic_search(query: str, index: faiss.IndexFlatL2, top_n=10) -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Perform FAISS vector search and return ranked results.\n",
    "    \"\"\"\n",
    "    query_embedding = np.array(get_embedding(query)).reshape(1, -1)\n",
    "    _, vector_top_n = index.search(query_embedding, top_n)  # Retrieve top vector matches\n",
    "\n",
    "    return {i.item(): rank+1 for rank, i in enumerate(vector_top_n[0])}  # Rank position starts from 1\n",
    "\n",
    "def hybrid_search(query, bm25: BM25Okapi, index: faiss.IndexFlatL2, tag_embeddings: dict[str, faiss.IndexFlatL2], top_n=10, k=60, bm25_weight = 0, tag_weight=0.9):\n",
    "    \"\"\"\n",
    "    Perform hybrid search using Reciprocal Rank Fusion (RRF).\n",
    "    \"\"\"\n",
    "    bm25_results = keyword_search(query, bm25, top_n=top_n)\n",
    "    vector_results = semantic_search(query, index, top_n=top_n)\n",
    "    tag_results = tag_search(query, tag_embeddings, top_n=top_n)\n",
    "\n",
    "    # results = rrf_fusion({\"BM25\": bm25_results, \"Vector\": vector_results}, k=k)\n",
    "\n",
    "    weights = {\"BM25\": bm25_weight, \"Vector\": 1-bm25_weight - tag_weight, \"Tag\": tag_weight}\n",
    "    results = weighted_fusion({\"BM25\": bm25_results, \"Vector\": vector_results, \"Tag\": tag_results}, weights)\n",
    "    \n",
    "    return [(documents[i], score) for i, score in results]\n",
    "\n",
    "query = \"What courses are good for beginner programmers?\"\n",
    "results = hybrid_search(query, bm25, index, tag_embeddings)\n",
    "\n",
    "for rank, (doc, score) in enumerate(results, 1):\n",
    "    source = doc2source[doc]\n",
    "    print(f\"{rank}. { source.display} (type: {source.type}) (Score: {score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
